{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p00dRvFLuhKW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/content/f1_pitstops_2018_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "7vIKyuJmxWe6",
    "outputId": "a03883f2-6dcb-418b-eeb9-93caea043c5f"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 3. Exact mapping of *all* broken patterns you may have\n",
    "# -------------------------------------------------\n",
    "name_fix = {\n",
    "    # Kimi RÃ¤ikkÃ¶nen\n",
    "    'Kimi RÃƒÆ’Ã‚Â¤ikkÃƒÆ’Ã‚Â¶nen' : 'Kimi RÃ¤ikkÃ¶nen',\n",
    "    'Kimi RÃƒÂ¤ikkÃƒÂ¶nen': 'Kimi RÃ¤ikkÃ¶nen',\n",
    "    'Kimi RÃƒÆ’Ã‚Â¤ikkÃƒÆ’Ã‚Â¶nen': 'Kimi RÃ¤ikkÃ¶nen',\n",
    "    'Kimi RÃƒÂ¤ikkÃƒÂ¶nen': 'Kimi RÃ¤ikkÃ¶nen',\n",
    "    'Kimi RÃƒÂ¤ikkÃƒÂ¶nen': 'Kimi RÃ¤ikkÃ¶nen',\n",
    "\n",
    "    # Nico HÃ¼lkenberg\n",
    "    'Nico HÃƒÂ¼lkenberg':   'Nico HÃ¼lkenberg',\n",
    "    'Nico HÃƒÆ’Ã‚Â¼lkenberg': 'Nico HÃ¼lkenberg',\n",
    "    'Nico HÃƒÂ¼lkenberg':   'Nico HÃ¼lkenberg',\n",
    "\n",
    "    # Sergio PÃ©rez\n",
    "    'Sergio PÃƒÂ©rez':      'Sergio PÃ©rez',\n",
    "    'Sergio PÃƒÆ’Ã‚Â©rez':    'Sergio PÃ©rez',\n",
    "    'Sergio PÃƒÂ©rez':      'Sergio PÃ©rez',\n",
    "\n",
    "    # Add any others you discover\n",
    "}\n",
    "\n",
    "df_raw['Driver'] = df_raw['Driver'].replace(name_fix, regex=False)\n",
    "\n",
    "print(\"\\nAfter replacement â€“ unique driver names (first 15):\")\n",
    "print(sorted(df_raw['Driver'].unique())[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKNxsBX0yUSb",
    "outputId": "3017c347-ab52-4da6-ae72-69c0dadaf4a0"
   },
   "outputs": [],
   "source": [
    "print(df['Driver'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4qN0cMIybwj"
   },
   "outputs": [],
   "source": [
    "name_corrections = {\n",
    "    'Kimi RÃƒÆ’Ã‚Â¤ikkÃƒÆ’Ã‚Â¶nen' : 'Kimi RÃ¤ikkÃ¶nen',\n",
    "    'Nico HÃƒÆ’Ã‚Â¼lkenberg' : 'Nico HÃ¼lkenberg',\n",
    "    'Sergio PÃƒÆ’Ã‚Â©rez' : 'Sergio PÃ©rez',\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrE6J4CSzP8m"
   },
   "outputs": [],
   "source": [
    "df['Driver'] = df['Driver'].replace(name_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "el8V4sho0yqY",
    "outputId": "be30914e-f2e7-4953-ee69-785cffcba80b"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WqMjXrw0zlb",
    "outputId": "a90c48e6-a155-459f-e4b1-8abe7bf2a7f8"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lppca8dP1PtD",
    "outputId": "4859caca-85f4-431c-aac9-0a4078698904"
   },
   "outputs": [],
   "source": [
    "df['Time_of_race'] = pd.to_datetime(df['Time_of_race'], errors='coerce')\n",
    "df = df.drop(columns=['Abbreviation', 'Total Pit Stops'])\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDENgrrX7jkk"
   },
   "outputs": [],
   "source": [
    "df['Stint'] = df['Stint'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvukCLXS8Eoo"
   },
   "outputs": [],
   "source": [
    "df['Pit_Lap'] = df['Pit_Lap'].replace('', pd.NA)\n",
    "df['Pit_Lap'] = df['Pit_Lap'].fillna(df['Laps'] - df['Stint Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5lxD9riQ9C6D",
    "outputId": "8bad8950-6ac1-43c8-ecf4-eb0aa0911b5a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bp-jPLYj9YGf",
    "outputId": "25cc047a-b120-4d30-aa7b-8d9d0894605b"
   },
   "outputs": [],
   "source": [
    "df['Pit_Lap'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKytPINj9zlW",
    "outputId": "456d711e-f3b6-463d-fada-a869fb1f6291"
   },
   "outputs": [],
   "source": [
    "(df['Pit_Lap'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJNtlqM994GK",
    "outputId": "a36a9cbd-8842-4065-c3d1-d98431dcbe84"
   },
   "outputs": [],
   "source": [
    "df['Pit_Lap'].isna().sum() + (df['Pit_Lap'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "0lFOhSXp9_i4",
    "outputId": "69c27e5a-f1d9-4954-cfde-e5ae06a02835"
   },
   "outputs": [],
   "source": [
    "df[df['Pit_Lap'].isna() | (df['Pit_Lap'] == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN0hwMV7-B7y"
   },
   "outputs": [],
   "source": [
    "# Replace NaN in Pit_Lap using (Laps - Stint Length)\n",
    "df.loc[df['Pit_Lap'].isna(), 'Pit_Lap'] = df['Laps'] - df['Stint Length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoxpaBkJ-jlm",
    "outputId": "2bb8d2f5-f334-4317-ba52-b743b925497a"
   },
   "outputs": [],
   "source": [
    "print(df['Pit_Time'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7NkZ3OG-kPF",
    "outputId": "602e071f-c0b3-4e2a-dfae-84a0514c73b4"
   },
   "outputs": [],
   "source": [
    "(df['Circuit'] == '').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "91BXwFgWw_b1",
    "outputId": "1f5f87e9-622b-46c0-d33b-277a1ea7a0b1"
   },
   "outputs": [],
   "source": [
    "df[df['Race Name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2eTKmuI4f_X",
    "outputId": "f34e5f9f-6bc5-4a61-c6a0-e5115c7567d2"
   },
   "outputs": [],
   "source": [
    "print(df['Circuit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bO2dB7GK7tUr"
   },
   "outputs": [],
   "source": [
    "name_corrections = {\n",
    "    'AutÃƒÆ’Ã‚Â³dromo Hermanos RodrÃƒÆ’Ã‚Â­guez' : 'AutÃ³dromo Hermanos RodrÃ­guez',\n",
    "    'NÃƒÆ’Ã‚Â¼rburgring' : 'NÃ¼rburgring',\n",
    "    'AutÃƒÆ’Ã‚Â³dromo Internacional do Algarve' : 'AutÃ³dromo Internacional do Algarve',\n",
    "    'AutÃƒÆ’Ã‚Â³dromo JosÃƒÆ’Ã‚Â© Carlos Pace' : 'Interlagos'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n479HjnG7sUW"
   },
   "outputs": [],
   "source": [
    "df['Circuit'] = df['Circuit'].replace(name_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZQ3J6hQ9BJs",
    "outputId": "632962e5-555b-4f0b-93df-56aa71616693"
   },
   "outputs": [],
   "source": [
    "print(df['Tire Compound'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6dM-FPd9MVh"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Race Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "m5DfiaB3AQzG",
    "outputId": "e4860969-9753-4564-db52-2aff3f5a0a08"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skkrDQQKCFpv",
    "outputId": "53e122c5-0be9-4b11-868a-2e9f6e70e3f1"
   },
   "outputs": [],
   "source": [
    "cols = ['Date', 'Time_of_race', 'Country', 'Location',\n",
    "        'AvgPitStopTime', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH']\n",
    "\n",
    "# Check if all columns have NaN in the same rows\n",
    "same_rows = df[cols].isna().all(axis=1)  # rows where *all* are NaN\n",
    "total_same = same_rows.sum()\n",
    "\n",
    "print(f\"âœ… Number of rows where ALL these columns are NaN: {total_same}\")\n",
    "#print(f\"Total NaN in each column: {df[cols].isna().sum().iloc[0]}\")\n",
    "print(\"âž¡ï¸ Do they match exactly?\", total_same == df[cols].isna().sum().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arSRDwbeEPYM",
    "outputId": "0f86d135-edc2-4bfa-882c-8e5956ee4c72"
   },
   "outputs": [],
   "source": [
    "cols = ['AvgPitStopTime', 'Lap Time Variation', 'Fast Lap Attempts', 'Driver Aggression Score']\n",
    "\n",
    "# Check if all columns have NaN in the same rows\n",
    "same_rows = df[cols].isna().all(axis=1)  # rows where *all* are NaN\n",
    "total_same = same_rows.sum()\n",
    "\n",
    "print(f\"âœ… Number of rows where ALL these columns are NaN: {total_same}\")\n",
    "print(f\"Total NaN in each column: {df[cols].isna().sum().iloc[0]}\")\n",
    "print(\"âž¡ï¸ Do they match exactly?\", total_same == df[cols].isna().sum().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHe_8wHUFXFL",
    "outputId": "604fd863-7026-4168-c44f-aac2fbaa772b"
   },
   "outputs": [],
   "source": [
    "# Define your two column groups\n",
    "group1 = ['AvgPitStopTime', 'Lap Time Variation', 'Fast Lap Attempts', 'Driver Aggression Score']\n",
    "group2 = ['Date', 'Time_of_race', 'Country', 'Location',\n",
    "          'AvgPitStopTime', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH']\n",
    "\n",
    "# Step 1: Find the 185 rows where all 4 group1 columns are NaN\n",
    "mask_group1 = df[group1].isna().all(axis=1)\n",
    "\n",
    "# Step 2: Among those rows, check how many also have all group2 columns as NaN\n",
    "mask_group2 = df[group2].isna().all(axis=1)\n",
    "\n",
    "# Step 3: Compare overlap\n",
    "overlap_count = (mask_group1 & mask_group2).sum()\n",
    "total_group1 = mask_group1.sum()\n",
    "\n",
    "print(f\"âœ… Rows where all 4 group1 columns are NaN: {total_group1}\")\n",
    "print(f\"âœ… Rows where those same rows also have all 8 group2 columns NaN: {overlap_count}\")\n",
    "print(f\"âž¡ï¸ Do they match exactly? {overlap_count == total_group1}\")\n",
    "\n",
    "# (Optional) Display overlapping rows\n",
    "# df[mask_group1 & mask_group2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "1zCpmqbXF3Bl",
    "outputId": "37b752a7-892a-404b-8412-662185eb653f"
   },
   "outputs": [],
   "source": [
    "df[mask_group1 & mask_group2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkIT67NhGVOy",
    "outputId": "30bda9fd-7087-4b38-bc29-64c90288d30e"
   },
   "outputs": [],
   "source": [
    "# Drop rows where all 4 group1 columns AND all 8 group2 columns are NaN\n",
    "df = df.drop(df[mask_group1 & mask_group2].index)\n",
    "\n",
    "print(\"âœ… Dropped rows where both group1 and group2 columns are completely NaN.\")\n",
    "print(f\"Remaining rows in dataframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnQTLGQ6GpO-"
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "AAmpjEWbGsSy",
    "outputId": "e6359ee3-737e-46c3-eb30-c6d9dbb5fe45"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KCe7HrIGtIT",
    "outputId": "5ca11162-3899-465e-87b5-7bca93cf31c5"
   },
   "outputs": [],
   "source": [
    "# Show rows where Country or Location are missing\n",
    "missing_rows = df[df['Country'].isna() | df['Location'].isna()][['Circuit', 'Country', 'Location']]\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weunqLpaIO-5",
    "outputId": "6037dd65-6cfe-4724-a0f9-6550a89b09ef"
   },
   "outputs": [],
   "source": [
    "print(df['Circuit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5ikztqaIg2T",
    "outputId": "70b4d289-eb41-4b64-9678-3bfb5596bb9b"
   },
   "outputs": [],
   "source": [
    "print(df['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsK2HQvKIlFe",
    "outputId": "1daf61c5-ff80-4340-c03e-7b00d89cf524"
   },
   "outputs": [],
   "source": [
    "print(df['Location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4qx-ILEInxx"
   },
   "outputs": [],
   "source": [
    "# Example: fill based on Circuit\n",
    "df.loc[df['Circuit'] == 'Albert Park Grand Prix Circuit', ['Country', 'Location']] = ['Australia', 'Melbourne']\n",
    "df.loc[df['Circuit'] == 'Bahrain International Circuit', ['Country', 'Location']] = ['Bahrain', 'Sakhir']\n",
    "df.loc[df['Circuit'] == 'Shanghai International Circuit', ['Country', 'Location']] = ['China', 'Shanghai']\n",
    "df.loc[df['Circuit'] == 'Baku City Circuit', ['Country', 'Location']] = ['Azerbaijan', 'Baku']\n",
    "df.loc[df['Circuit'] == 'Circuit de Barcelona-Catalunya', ['Country', 'Location']] = ['Spain', 'MontmelÃƒÂ³']\n",
    "df.loc[df['Circuit'] == 'Circuit de Monaco', ['Country', 'Location']] = ['Monaco', 'Monte-Carlo']\n",
    "df.loc[df['Circuit'] == 'Circuit Gilles Villeneuve', ['Country', 'Location']] = ['Canada', 'Montreal']\n",
    "df.loc[df['Circuit'] == 'Circuit Paul Ricard', ['Country', 'Location']] = ['France', 'Le Castellet']\n",
    "df.loc[df['Circuit'] == 'Red Bull Ring', ['Country', 'Location']] = ['Austria', 'Spielberg']\n",
    "df.loc[df['Circuit'] == 'Silverstone Circuit', ['Country', 'Location']] = ['UK', 'Silverstone']\n",
    "df.loc[df['Circuit'] == 'Hockenheimring', ['Country', 'Location']] = ['Germany', 'Hockenheim']\n",
    "df.loc[df['Circuit'] == 'Hungaroring', ['Country', 'Location']] = ['Hungary', 'Budapest']\n",
    "df.loc[df['Circuit'] == 'Circuit de Spa-Francorchamps', ['Country', 'Location']] = ['Belgium', 'Spa']\n",
    "df.loc[df['Circuit'] == 'Autodromo Nazionale di Monza', ['Country', 'Location']] = ['Italy', 'Monza']\n",
    "df.loc[df['Circuit'] == 'Marina Bay Street Circuit', ['Country', 'Location']] = ['Singapore', 'Marina Bay']\n",
    "df.loc[df['Circuit'] == 'Sochi Autodrom', ['Country', 'Location']] = ['Russia', 'Sochi']\n",
    "df.loc[df['Circuit'] == 'Suzuka Circuit', ['Country', 'Location']] = ['Japan', 'Suzuka']\n",
    "df.loc[df['Circuit'] == 'Circuit of the Americas', ['Country', 'Location']] = ['USA', 'Austin']\n",
    "df.loc[df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez', ['Country', 'Location']] = ['Mexico', 'Mexico City']\n",
    "df.loc[df['Circuit'] == 'Yas Marina Circuit', ['Country', 'Location']] = ['UAE', 'Abu Dhabi']\n",
    "df.loc[df['Circuit'] == 'Autodromo Internazionale del Mugello', ['Country', 'Location']] = ['Italy', 'Mugello']\n",
    "df.loc[df['Circuit'] == 'NÃ¼rburgring', ['Country', 'Location']] = ['Germany', 'NÃ¼rburg']\n",
    "df.loc[df['Circuit'] == 'AutÃ³dromo Internacional do Algarve', ['Country', 'Location']] = ['Portugal', 'PortimÃ£o']\n",
    "df.loc[df['Circuit'] == 'Autodromo Enzo e Dino Ferrari', ['Country', 'Location']] = ['Italy', 'Imola']\n",
    "df.loc[df['Circuit'] == 'Istanbul Park', ['Country', 'Location']] = ['Turkey', 'Istanbul']\n",
    "df.loc[df['Circuit'] == 'Circuit Park Zandvoort', ['Country', 'Location']] = ['Netherlands', 'Zandvoort']\n",
    "df.loc[df['Circuit'] == 'Losail International Circuit', ['Country', 'Location']] = ['Qatar', 'Al Daayen']\n",
    "df.loc[df['Circuit'] == 'Jeddah Corniche Circuit', ['Country', 'Location']] = ['Saudi Arabia', 'Jeddah']\n",
    "df.loc[df['Circuit'] == 'Miami International Autodrome', ['Country', 'Location']] = ['United States', 'Miami']\n",
    "df.loc[df['Circuit'] == 'Las Vegas Strip Street Circuit', ['Country', 'Location']] = ['United States', 'Las Vegas']\n",
    "df.loc[df['Circuit'] == 'Interlagos', ['Country', 'Location']] = ['Brazil', 'SÃ£o Paulo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ktpu6IrMQnF1",
    "outputId": "3943aa12-d2a5-4371-c770-51b0b193fe6b"
   },
   "outputs": [],
   "source": [
    "print(df['Location'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70lyCRz0Q1y1",
    "outputId": "ec9a9b81-c3b4-496e-8798-8fbd161e6e62"
   },
   "outputs": [],
   "source": [
    "print(df['Season'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 774
    },
    "id": "JfyKIUqSs6JP",
    "outputId": "afad8739-d3ad-4ba6-eca4-a4f3eb3cd873"
   },
   "outputs": [],
   "source": [
    "df[df['Date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3j_TjZ9tfGJ",
    "outputId": "ad970c5c-1a87-4240-d5b2-ee146991e604"
   },
   "outputs": [],
   "source": [
    "missing_dates = df[df['Date'].isna()][['Season', 'Circuit']]\n",
    "print(missing_dates.drop_duplicates().sort_values(['Season', 'Circuit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNyp5bN9vn_E",
    "outputId": "0bf09983-3e45-48c3-86cc-fcd3189079ec"
   },
   "outputs": [],
   "source": [
    "# Group by year and list unique circuits\n",
    "circuits_per_year = df.groupby('Season')['Circuit'].unique()\n",
    "\n",
    "# Print in a readable format\n",
    "for year, circuits in circuits_per_year.items():\n",
    "    print(f\"\\nðŸŽï¸ {year} Circuits ({len(circuits)} total):\")\n",
    "    for c in circuits:\n",
    "        print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtRt1qTKw5At",
    "outputId": "dc03e3f4-d1a8-4fee-9cc1-def93d648b69"
   },
   "outputs": [],
   "source": [
    "# Step 1: Identify missing or NaT dates\n",
    "missing_dates = df[df['Date'].isna()][['Season', 'Circuit']]\n",
    "print(\"Total missing/NaT dates:\", len(missing_dates))\n",
    "print(missing_dates.drop_duplicates().sort_values(['Season', 'Circuit']))\n",
    "\n",
    "# Step 2: Fill missing values manually using Year + Circuit\n",
    "# Example: Replace below with actual race dates\n",
    "\n",
    "#2018\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Albert Park Grand Prix Circuit'), 'Date'] = '25-03-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '08-04-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Shanghai International Circuit'), 'Date'] = '15-04-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '29-04-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '13-05-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '27-05-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit Gilles Villeneuve'), 'Date'] = '10-06-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit Paul Ricard'), 'Date'] = '24-06-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '01-07-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '08-07-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Hockenheimring'), 'Date'] = '22-07-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '29-07-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '26-08-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '02-09-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Marina Bay Street Circuit'), 'Date'] = '16-09-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Sochi Autodrom'), 'Date'] = '30-09-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Suzuka Circuit'), 'Date'] = '07-10-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '21-10-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez'), 'Date'] = '29-10-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Interlagos'), 'Date'] = '11-11-2018'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '25-11-2018'\n",
    "\n",
    "#2019\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Albert Park Grand Prix Circuit'), 'Date'] = '17-03-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '31-03-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Shanghai International Circuit'), 'Date'] = '14-04-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '28-04-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '12-05-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '26-05-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit Gilles Villeneuve'), 'Date'] = '09-06-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit Paul Ricard'), 'Date'] = '23-06-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '30-06-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '14-07-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Hockenheimring'), 'Date'] = '28-07-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '04-08-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '01-09-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '08-09-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Marina Bay Street Circuit'), 'Date'] = '22-09-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Sochi Autodrom'), 'Date'] = '29-09-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Suzuka Circuit'), 'Date'] = '13-10-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez'), 'Date'] = '28-10-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '04-11-2019'\n",
    "df.loc[(df['Season'] == 2018) & (df['Circuit'] == 'Interlagos'), 'Date'] = '17-11-2019'\n",
    "df.loc[(df['Season'] == 2019) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '01-12-2019'\n",
    "\n",
    "#2020\n",
    "#df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '05-07-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '19-07-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '02-08-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '16-08-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '30-08-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '06-09-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Autodromo Internazionale del Mugello'), 'Date'] = '13-09-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Sochi Autodrom'), 'Date'] = '27-09-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'NÃ¼rburgring'), 'Date'] = '11-10-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'AutÃ³dromo Internacional do Algarve'), 'Date'] = '25-10-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Autodromo Enzo e Dino Ferrari'), 'Date'] = '01-11-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Istanbul Park'), 'Date'] = '15-11-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '29-11-2020'\n",
    "df.loc[(df['Season'] == 2020) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '13-12-2020'\n",
    "\n",
    "#2021\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '28-03-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Autodromo Enzo e Dino Ferrari'), 'Date'] = '18-04-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'AutÃ³dromo Internacional do Algarve'), 'Date'] = '02-05-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '09-05-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '23-05-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '06-06-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit Paul Ricard'), 'Date'] = '20-06-2021'\n",
    "#df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = ''\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '18-07-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '01-08-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '29-08-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit Park Zandvoort'), 'Date'] = '05-09-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '12-09-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Sochi Autodrom'), 'Date'] = '26-09-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Istanbul Park'), 'Date'] = '10-10-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '25-10-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Losail International Circuit'), 'Date'] = '21-10-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Jeddah Corniche Circuit'), 'Date'] = '05-12-2021'\n",
    "df.loc[(df['Season'] == 2021) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '12-12-2021'\n",
    "\n",
    "#2022\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '20-03-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Jeddah Corniche Circuit'), 'Date'] = '27-03-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Albert Park Grand Prix Circuit'), 'Date'] = '10-04-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Autodromo Enzo e Dino Ferrari'), 'Date'] = '24-04-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Miami International Autodrome'), 'Date'] = '09-05-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '22-05-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '29-05-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '12-06-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit Gilles Villeneuve'), 'Date'] = '19-06-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '03-07-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '10-07-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit Paul Ricard'), 'Date'] = '24-07-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '31-07-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '28-08-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit Park Zandvoort'), 'Date'] = '04-09-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '11-09-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Marina Bay Street Circuit'), 'Date'] = '02-10-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Suzuka Circuit'), 'Date'] = '09-10-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '24-10-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez'), 'Date'] = '31-10-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Interlagos'), 'Date'] = '13-11-2022'\n",
    "df.loc[(df['Season'] == 2022) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '20-11-2022'\n",
    "\n",
    "#2023\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '05-03-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Jeddah Corniche Circuit'), 'Date'] = '19-03-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Albert Park Grand Prix Circuit'), 'Date'] = '02-04-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '30-04-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Miami International Autodrome'), 'Date'] = '08-05-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '28-05-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '04-06-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit Gilles Villeneuve'), 'Date'] = '18-06-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '02-07-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '09-07-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '23-07-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '30-07-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit Park Zandvoort'), 'Date'] = '27-08-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '03-09-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Marina Bay Street Circuit'), 'Date'] = '17-09-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Suzuka Circuit'), 'Date'] = '24-09-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Losail International Circuit'), 'Date'] = '08-10-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '23-10-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez'), 'Date'] = '30-10-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Interlagos'), 'Date'] = '05-11-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Las Vegas Strip Street Circuit'), 'Date'] = '19-11-2023'\n",
    "df.loc[(df['Season'] == 2023) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '26-11-2023'\n",
    "\n",
    "#2024\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Bahrain International Circuit'), 'Date'] = '02-03-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Jeddah Corniche Circuit'), 'Date'] = '09-03-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Albert Park Grand Prix Circuit'), 'Date'] = '24-03-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Suzuka Circuit'), 'Date'] = '07-04-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Shanghai International Circuit'), 'Date'] = '21-04-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Miami International Autodrome'), 'Date'] = '06-05-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Autodromo Enzo e Dino Ferrari'), 'Date'] = '19-05-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit de Monaco'), 'Date'] = '26-05-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit Gilles Villeneuve'), 'Date'] = '09-06-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit de Barcelona-Catalunya'), 'Date'] = '23-06-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Red Bull Ring'), 'Date'] = '30-06-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Silverstone Circuit'), 'Date'] = '07-07-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Hungaroring'), 'Date'] = '21-07-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit de Spa-Francorchamps'), 'Date'] = '28-07-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit Park Zandvoort'), 'Date'] = '25-08-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Autodromo Nazionale di Monza'), 'Date'] = '01-09-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Baku City Circuit'), 'Date'] = '15-09-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Marina Bay Street Circuit'), 'Date'] = '22-09-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Circuit of the Americas'), 'Date'] = '21-10-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'AutÃ³dromo Hermanos RodrÃ­guez'), 'Date'] = '28-10-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Interlagos'), 'Date'] = '03-11-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Las Vegas Strip Street Circuit'), 'Date'] = '24-11-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Losail International Circuit'), 'Date'] = '01-12-2024'\n",
    "df.loc[(df['Season'] == 2024) & (df['Circuit'] == 'Yas Marina Circuit'), 'Date'] = '08-12-2024'\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Ensure all values are proper datetime objects\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# Step 4: Check for any remaining NaT values\n",
    "remaining_nat = df[df['Date'].isna()]\n",
    "print(\"Remaining rows with NaT:\", len(remaining_nat))\n",
    "if not remaining_nat.empty:\n",
    "    print(remaining_nat[['Year', 'Circuit']].drop_duplicates().sort_values(['Year', 'Circuit']))\n",
    "\n",
    "# (Optional) Step 5: Save cleaned data\n",
    "# df.to_csv(\"f1_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMwWm9KUCQle",
    "outputId": "8e7cdb1d-9fca-42cc-99ed-9763d0a8d329"
   },
   "outputs": [],
   "source": [
    "# Find cells with exactly empty string (\"\")\n",
    "empty_cells = (df == ' ').sum().sum()\n",
    "\n",
    "print(f\"Total empty-string cells in DataFrame: {empty_cells}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "3vecp6ohEAeK",
    "outputId": "d84a7123-b8da-4515-a858-6e35f3f6ad20"
   },
   "outputs": [],
   "source": [
    "df[df['Pit_Time'].isna() | (df['Pit_Time'].astype(str).str.strip() == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "LRqRSBGrKUyT",
    "outputId": "f505b5fe-9113-4988-bbab-ae17cad188d2"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'AvgPitStopTime', 'Time_of_race', 'Air_Temp_C', 'Track_Temp_C',\n",
    "    'Humidity_%', 'Wind_Speed_KMH', 'Lap Time Variation', 'Tire Usage Aggression',\n",
    "    'Fast Lap Attempts', 'Driver Aggression Score', 'Tire Compound',\n",
    "    'Stint Length', 'Pit_Lap', 'Pit_Time'\n",
    "]\n",
    "\n",
    "df[df[cols].isna().all(axis=1) | (df[cols].astype(str).apply(lambda x: x.str.strip() == '').all(axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9OUk1eQMzwj"
   },
   "outputs": [],
   "source": [
    "df['Location'] = df['Location'].replace('MontmelÃƒÂ³', 'MontmelÃ³')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "UDiECvb-QV3i",
    "outputId": "2f264653-52fc-4655-f565-b56d7a185d2e"
   },
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df.to_csv('cleaned_pitstops.csv', index=False)\n",
    "\n",
    "# For Colab users â€” this lets you download directly\n",
    "from google.colab import files\n",
    "files.download('cleaned_pitstops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49Ifk8D3RMex",
    "outputId": "b7922856-5e1b-4ba2-8f11-e904ef2694b4"
   },
   "outputs": [],
   "source": [
    "print(df['Time_of_race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N2wMceUUdUA"
   },
   "outputs": [],
   "source": [
    "# Keep only the time portion from the datetime\n",
    "df['Time_of_race'] = pd.to_datetime(df['Time_of_race'], errors='coerce').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzr6As-LU58l",
    "outputId": "4dfe8d4f-0744-4975-9cb3-d15a143a9eab"
   },
   "outputs": [],
   "source": [
    "print(df['Time_of_race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "aXgjP6tVVJWV",
    "outputId": "42e089d0-44fa-4520-ac49-a5ef173ef044"
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_pitstops.csv', index=False)\n",
    "\n",
    "# For Colab users â€” this lets you download directly\n",
    "from google.colab import files\n",
    "files.download('cleaned_pitstops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMuPVo91VomD",
    "outputId": "5df16267-5b1b-4353-b998-0db5182cd599"
   },
   "outputs": [],
   "source": [
    "# Count NaN or missing values\n",
    "missing_count = df['Time_of_race'].isna().sum()\n",
    "print(\"Number of missing or NaN values in Time_of_race:\", missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hFRmOcpWgR8"
   },
   "outputs": [],
   "source": [
    "# Create a lookup dictionary from rows where Time_of_race is not missing\n",
    "lookup = (\n",
    "    df.dropna(subset=['Time_of_race'])\n",
    "      .set_index(['Season', 'Round', 'Circuit'])['Time_of_race']\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# Fill missing Time_of_race using the lookup\n",
    "def fill_time(row):\n",
    "    if pd.isna(row['Time_of_race']) or row['Time_of_race'] == '':\n",
    "        key = (row['Season'], row['Round'], row['Circuit'])\n",
    "        return lookup.get(key, row['Time_of_race'])\n",
    "    return row['Time_of_race']\n",
    "\n",
    "df['Time_of_race'] = df.apply(fill_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1vGXy42WjGy",
    "outputId": "a71b1bed-54ef-49f9-fcee-1746d2836172"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Time_of_race'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdJalnCbXV2S",
    "outputId": "ff9f142a-98dd-4516-9543-7fa67c3b55c4"
   },
   "outputs": [],
   "source": [
    "print(df['Time_of_race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVF6UpewXvq4",
    "outputId": "a0816684-391e-458f-ea94-19d2b676daf5"
   },
   "outputs": [],
   "source": [
    "# Find rows where Time_of_race is missing or empty\n",
    "missing_time_rows = df[df['Time_of_race'].isna() | (df['Time_of_race'] == '')]\n",
    "\n",
    "# Get unique combinations of Season, Round, and Circuit\n",
    "unique_missing_races = missing_time_rows[['Season', 'Round', 'Circuit']].drop_duplicates()\n",
    "\n",
    "# Display them\n",
    "print(\"ðŸ Unique races with missing Time_of_race:\", len(unique_missing_races))\n",
    "print(unique_missing_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8egteDhYReP",
    "outputId": "785e1e2f-4b65-4cba-d3de-360af2b65b00"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary mapping (season, round, circuit) â†’ time\n",
    "time_updates = {\n",
    "    (2018, 19, 'AutÃ³dromo Hermanos RodrÃ­guez'): '19:10:00',\n",
    "    (2019, 18, 'AutÃ³dromo Hermanos RodrÃ­guez'): '19:10:00',\n",
    "    (2020, 11, 'NÃ¼rburgring'): '12:10:00',\n",
    "    (2020, 12, 'AutÃ³dromo Internacional do Algarve'): '13:10:00',\n",
    "    (2021, 3, 'AutÃ³dromo Internacional do Algarve'): '14:00:00',\n",
    "    (2022, 20, 'AutÃ³dromo Hermanos RodrÃ­guez'): '20:00:00',\n",
    "    (2023, 19, 'AutÃ³dromo Hermanos RodrÃ­guez'): '20:00:00',\n",
    "    (2024, 20, 'AutÃ³dromo Hermanos RodrÃ­guez'): '20:00:00',\n",
    "    (2024, 21, 'Interlagos'): '02:30:00'\n",
    "}\n",
    "\n",
    "# Fill missing times based on these combinations\n",
    "for (season, rnd, circuit), race_time in time_updates.items():\n",
    "    mask = (\n",
    "        (df['Season'] == season) &\n",
    "        (df['Round'] == rnd) &\n",
    "        (df['Circuit'] == circuit)\n",
    "    )\n",
    "    df.loc[mask, 'Time_of_race'] = race_time\n",
    "\n",
    "# (Optional) Verify updated rows\n",
    "print(df.loc[df['Time_of_race'] == '05:10:00', ['Season', 'Round', 'Circuit', 'Time_of_race']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpzbHyBpgxOa",
    "outputId": "8f8ba875-55c9-48d2-d5b4-a8600902ee80"
   },
   "outputs": [],
   "source": [
    "# Find rows where Time_of_race is missing or empty\n",
    "missing_time_rows = df[df['Time_of_race'].isna() | (df['Time_of_race'] == '')]\n",
    "\n",
    "# Get unique combinations of Season, Round, and Circuit\n",
    "unique_missing_races = missing_time_rows[['Season', 'Round', 'Circuit']].drop_duplicates()\n",
    "\n",
    "# Display them\n",
    "print(\"ðŸ Unique races with missing Time_of_race:\", len(unique_missing_races))\n",
    "print(unique_missing_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nojGic03g2_m",
    "outputId": "29198d48-51ef-4c28-a94a-9496a9ec03fc"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Time_of_race'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6Nqm5aGihJ-",
    "outputId": "5fdb169a-2326-4471-b3d2-4ee5846e360d"
   },
   "outputs": [],
   "source": [
    "# Find rows where Time_of_race is missing or empty\n",
    "missing_time_rows = df[df['Driver Aggression Score'].isna() | (df['Driver Aggression Score'] == '')]\n",
    "\n",
    "# Get unique combinations of Season, Round, and Circuit\n",
    "unique_missing_races = missing_time_rows[['Season', 'Round', 'Circuit']].drop_duplicates()\n",
    "\n",
    "# Display them\n",
    "print(\"ðŸ Unique races with missing Time_of_race:\", len(unique_missing_races))\n",
    "print(unique_missing_races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJMyv4Gkjlu9",
    "outputId": "9b9d9494-3873-44f9-df4d-1d57969bae27"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 2. Define the four columns\n",
    "# -------------------------------------------------\n",
    "weather_cols = ['AvgPitStopTime', 'Lap Time Variation', 'Fast Lap Attempts', 'Driver Aggression Score']\n",
    "# -------------------------------------------------\n",
    "# 3. Count missing / empty / NaN per column\n",
    "# -------------------------------------------------\n",
    "missing_report = {}\n",
    "\n",
    "for col in weather_cols:\n",
    "    # 1. NaN (pandas null)\n",
    "    nan_cnt = df[col].isna().sum()\n",
    "\n",
    "    # 2. Empty strings (after converting to string)\n",
    "    empty_cnt = (df[col].astype(str).str.strip() == '').sum()\n",
    "\n",
    "    # 3. Total problematic rows\n",
    "    total_bad = nan_cnt + empty_cnt\n",
    "\n",
    "    missing_report[col] = {\n",
    "        'NaN'        : nan_cnt,\n",
    "        'Empty_str'  : empty_cnt,\n",
    "        'Total_bad'  : total_bad,\n",
    "        'Pct_bad'    : total_bad / len(df) * 100\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Pretty-print the report\n",
    "# -------------------------------------------------\n",
    "import json, math\n",
    "print(\"\\n=== Missing-value report for weather columns ===\\n\")\n",
    "for col, stats in missing_report.items():\n",
    "    print(f\"{col:15} | NaN: {stats['NaN']:>6} | Empty: {stats['Empty_str']:>6} | \"\n",
    "          f\"Total bad: {stats['Total_bad']:>6} | \"\n",
    "          f\"({stats['Pct_bad']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hm_00BHCpTWd",
    "outputId": "f0b5c61f-4876-4bb9-b65d-e5660d8df15f"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 5. Bonus: Rows that have *any* missing weather value\n",
    "# -------------------------------------------------\n",
    "rows_with_any_missing = df[weather_cols].isna().any(axis=1).sum()\n",
    "rows_all_missing      = df[weather_cols].isna().all(axis=1).sum()\n",
    "\n",
    "print(f\"\\nRows with at least one missing weather value : {rows_with_any_missing:,}\")\n",
    "print(f\"Rows with *all four* weather values missing  : {rows_all_missing:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJeTC6tUpWPi",
    "outputId": "3139ba8d-8d0b-41e6-96f0-8a0ffe3475de"
   },
   "outputs": [],
   "source": [
    "print(f\"Total rows in the dataset: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuoC_lGZpjKn",
    "outputId": "d125f5e5-503f-48ed-fbc7-b242281ef7f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "cols_to_check = [\n",
    "    'AvgPitStopTime',\n",
    "    'Lap Time Variation',\n",
    "    'Fast Lap Attempts',\n",
    "    'Driver Aggression Score'\n",
    "]\n",
    "# 3. Count rows where *ALL FOUR* are NaN\n",
    "all_missing_mask = df[cols_to_check].isna().all(axis=1)\n",
    "rows_to_drop = all_missing_mask.sum()\n",
    "\n",
    "print(f\"Rows where ALL FOUR columns are missing: {rows_to_drop:,}\")\n",
    "\n",
    "# 4. Drop ONLY those rows\n",
    "df_clean = df[~all_missing_mask].copy()\n",
    "\n",
    "print(f\"\\nBefore: {len(df):,} rows\")\n",
    "print(f\" After: {len(df_clean):,} rows  (removed {rows_to_drop:,} rows)\")\n",
    "\n",
    "# 5. Verify: no row has all four missing anymore\n",
    "still_all_missing = df_clean[cols_to_check].isna().all(axis=1).sum()\n",
    "print(f\"\\nRows with all four missing after drop: {still_all_missing}  (should be 0)\")\n",
    "\n",
    "# Optional: check how many rows still have *any* missing in these columns\n",
    "any_missing_after = df_clean[cols_to_check].isna().any(axis=1).sum()\n",
    "print(f\"Rows with at least one missing (kept): {any_missing_after:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_pKxjC1w-PX"
   },
   "outputs": [],
   "source": [
    "df = df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BxuGFnbvuTc",
    "outputId": "3c7df463-b53c-40a7-9d93-5b0096439edd"
   },
   "outputs": [],
   "source": [
    "print(f\"Total rows in the dataset: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4dgSOXHv1pz",
    "outputId": "131977f8-e8ee-48ff-a0dc-a6c0fc273f30"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Driver Aggression Score'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYNPLvBqwCBi",
    "outputId": "affad91f-c65f-4bf8-8c4e-8322e6b96308"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['AvgPitStopTime'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ds7AGsi7wY7o",
    "outputId": "0bc05f9c-41a6-4869-d4e6-926da2a351b0"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Lap Time Variation'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjNPzoH0wdS3",
    "outputId": "be2d43d3-b393-4559-fb79-c921393a8e60"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Fast Lap Attempts'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8V0meauwie4",
    "outputId": "b995b7ec-e4f8-44fe-a153-5d833111649a"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Pit_Time'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99oxMY-R0Wy9",
    "outputId": "0194a99c-c7fa-498b-b195-dd8c743ca083"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Stint Length'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7Zp2C9j1QoA",
    "outputId": "24689722-d2d0-4575-b66a-293271d0eb0c"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Tire Compound'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtKY7GMH1d1n",
    "outputId": "83a8b333-90a9-4342-d17a-b97023af7d14"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Tire Usage Aggression'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaB4GrPH1l5F",
    "outputId": "305ab27f-9637-4e19-9ef1-65193d38f258"
   },
   "outputs": [],
   "source": [
    "print(f\"Total rows in the dataset: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtW9qAAG2Evh",
    "outputId": "7b1bebff-27d1-4252-ee73-465c57267ea2"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Define the 4 columns you care about\n",
    "# -------------------------------------------------\n",
    "cols = [\n",
    "    'Tire Compound',\n",
    "    'Stint Length',\n",
    "    'Pit_Time',\n",
    "    'Pit_Lap'\n",
    "]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Count missing values per column\n",
    "# -------------------------------------------------\n",
    "missing_counts = df[cols].isna().sum()\n",
    "print(\"Missing value counts per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Check if ALL counts are equal\n",
    "# -------------------------------------------------\n",
    "all_same_count = missing_counts.nunique() == 1\n",
    "print(f\"\\nAll 4 columns have the SAME number of missing values? â†’ {all_same_count}\")\n",
    "\n",
    "if all_same_count:\n",
    "    n_missing = missing_counts.iloc[0]\n",
    "    print(f\"   â†’ Each column has {n_missing:,} missing values\")\n",
    "else:\n",
    "    print(\"   â†’ Counts differ â€” see above\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Check if missing values are in EXACTLY the same rows\n",
    "# -------------------------------------------------\n",
    "# Create a mask: True where ANY of the 4 is missing\n",
    "any_missing_mask = df[cols].isna().any(axis=1)\n",
    "\n",
    "# For each column, count how many of its missing rows are in the \"any missing\" set\n",
    "same_rows = all(\n",
    "    df[col].isna().sum() == (df[col].isna() & any_missing_mask).sum()\n",
    "    for col in cols\n",
    ")\n",
    "\n",
    "print(f\"\\nMissing values occur in the EXACT SAME ROWS? â†’ {same_rows}\")\n",
    "\n",
    "if same_rows:\n",
    "    print(f\"   â†’ All {n_missing:,} missing entries are in the same {n_missing:,} rows\")\n",
    "else:\n",
    "    print(\"   â†’ Some columns have missing values in different rows\")\n",
    "    # Show which rows differ (optional)\n",
    "    for col in cols:\n",
    "        diff = df[col].isna() & ~any_missing_mask\n",
    "        if diff.any():\n",
    "            print(f\"     â€¢ {col} has extra missing rows: {diff.sum()}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Bonus: Show the actual rows (first 5) with all 4 missing\n",
    "# -------------------------------------------------\n",
    "all_missing_rows = df[cols].isna().all(axis=1)\n",
    "if all_missing_rows.any():\n",
    "    print(f\"\\nSample rows where ALL 4 columns are missing (first 5):\")\n",
    "    print(df[all_missing_rows][['Season', 'Round', 'Driver'] + cols].head())\n",
    "else:\n",
    "    print(\"\\nNo rows where all 4 are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDzVc9EE3FYz",
    "outputId": "93d4c48c-bc50-4282-8432-fa2d12ece6d7"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Define the 4 columns\n",
    "# -------------------------------------------------\n",
    "cols = ['Tire Compound', 'Stint Length', 'Pit_Time', 'Pit_Lap']\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Verify they have the same number of missing values\n",
    "# -------------------------------------------------\n",
    "missing_counts = df[cols].isna().sum()\n",
    "print(\"Missing counts per column:\")\n",
    "print(missing_counts)\n",
    "\n",
    "all_same_count = missing_counts.nunique() == 1\n",
    "print(f\"\\nAll 4 columns have same missing count? â†’ {all_same_count}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Verify missing values are in the SAME rows\n",
    "# -------------------------------------------------\n",
    "any_missing = df[cols].isna().any(axis=1)\n",
    "same_rows = all(\n",
    "    (df[col].isna() & any_missing).sum() == df[col].isna().sum()\n",
    "    for col in cols\n",
    ")\n",
    "print(f\"Missing values in the SAME rows? â†’ {same_rows}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Drop rows where ALL 4 are missing\n",
    "# -------------------------------------------------\n",
    "all_missing_mask = df[cols].isna().all(axis=1)\n",
    "rows_to_drop = all_missing_mask.sum()\n",
    "print(f\"\\nRows where ALL 4 columns are missing: {rows_to_drop:,}\")\n",
    "\n",
    "df_clean = df[~all_missing_mask].copy()\n",
    "print(f\"Before drop: {len(df):,} rows\")\n",
    "print(f\" After drop: {len(df_clean):,} rows  (removed {rows_to_drop:,} rows)\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Final verification\n",
    "# -------------------------------------------------\n",
    "still_all_missing = df_clean[cols].isna().all(axis=1).sum()\n",
    "print(f\"\\nRows with all 4 missing after drop: {still_all_missing}  (should be 0)\")\n",
    "\n",
    "# Optional: Show how many rows still have partial missing (1â€“3 columns)\n",
    "partial_missing = df_clean[cols].isna().any(axis=1).sum() - still_all_missing\n",
    "print(f\"Rows with 1â€“3 missing (kept): {partial_missing:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzQWZHuk3wTW",
    "outputId": "024964a1-e0d0-4aa2-b914-4d2a2dde24e5"
   },
   "outputs": [],
   "source": [
    "df = df[~df[['Tire Compound', 'Stint Length', 'Pit_Time', 'Pit_Lap']].isna().all(axis=1)].copy()\n",
    "print(f\"Final rows: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQsW8tXE5VVx",
    "outputId": "3d74d3cc-d464-4bf7-ec26-3a24318afdd5"
   },
   "outputs": [],
   "source": [
    "print(f\"Total rows in the dataset: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0SqPTCN5dxI",
    "outputId": "751afff0-b226-4fdf-d978-34eeb0cea66c"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Pit_Lap'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HIIFsB55j_g",
    "outputId": "cd26c000-2c99-4dff-bc1b-33c3a2396931"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Wind_Speed_KMH'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "P27fYPBD559N",
    "outputId": "97f81d64-66ad-4a65-cf35-d9456abce7ed"
   },
   "outputs": [],
   "source": [
    "# Save as CSV\n",
    "df.to_csv('cleaned_pitstops.csv', index=False)\n",
    "\n",
    "# For Colab users â€” this lets you download directly\n",
    "from google.colab import files\n",
    "files.download('cleaned_pitstops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHCkVk1Q6qc_",
    "outputId": "87e09411-4802-493e-a479-015fd7edb1d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "cols = ['Air_Temp_C', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH']\n",
    "\n",
    "# 2. Find rows where ALL 4 are missing\n",
    "all_missing_mask = df[cols].isna().all(axis=1)\n",
    "missing_rows = df[all_missing_mask]\n",
    "\n",
    "print(f\"Found {len(missing_rows):,} rows where ALL 4 columns are missing.\\n\")\n",
    "\n",
    "# 3. Select only the columns we care about\n",
    "id_cols = ['Season', 'Round', 'Circuit']\n",
    "unique_races = missing_rows[id_cols].drop_duplicates()\n",
    "\n",
    "print(f\"Unique (Season, Round, Circuit) combinations: {len(unique_races)}\\n\")\n",
    "print(unique_races.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PRbR5I7j9tIW",
    "outputId": "40aebe24-6548-44c8-de51-03c71f5dc113"
   },
   "outputs": [],
   "source": [
    "# 1. Define the 4 columns that are missing together\n",
    "cols = ['Air_Temp_C', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH']\n",
    "\n",
    "# 2. Count how many such rows exist\n",
    "rows_to_remove = df[cols].isna().all(axis=1).sum()\n",
    "print(f\"Rows where ALL 4 columns are missing: {rows_to_remove:,}\")\n",
    "\n",
    "# 3. Drop those rows\n",
    "df_clean = df[~df[cols].isna().all(axis=1)].copy()\n",
    "\n",
    "print(f\"Before: {len(df):,} rows\")\n",
    "print(f\" After: {len(df_clean):,} rows  â†’ removed {rows_to_remove:,} rows\")\n",
    "\n",
    "# 4. Optional: Reassign back to df\n",
    "df = df_clean\n",
    "\n",
    "# 5. Final check: no row should have all 4 missing\n",
    "still_missing = df[cols].isna().all(axis=1).sum()\n",
    "print(f\"Rows with all 4 missing after cleanup: {still_missing}  (should be 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3_ByGgt1-3x",
    "outputId": "ce6e95e5-e9bc-4f43-8f91-c13f9ef25d11"
   },
   "outputs": [],
   "source": [
    "df = df[~df[['Air_Temp_C', 'Track_Temp_C', 'Humidity_%', 'Wind_Speed_KMH']].isna().all(axis=1)].copy()\n",
    "print(f\"Cleaned! Now {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgTuLbBk3gep",
    "outputId": "d4043a56-ad8e-4b33-85fa-72cb40924271"
   },
   "outputs": [],
   "source": [
    "print(\"Remaining missing times:\", df['Humidity_%'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "qE-xeUpr9R5u",
    "outputId": "d2ef2226-6e6f-447d-bb61-340aae9a5705"
   },
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_pitstops.csv', index=False)\n",
    "\n",
    "# For Colab users â€” this lets you download directly\n",
    "from google.colab import files\n",
    "files.download('cleaned_pitstops.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LrIheGTALLHw",
    "outputId": "d78d79c3-abc7-4e70-f567-f2b72815fe0b"
   },
   "outputs": [],
   "source": [
    "# Deep learning regression training script for predicting Driver Aggression Score\n",
    "# Run in a Python env with pandas, sklearn, matplotlib and tensorflow installed (or it will fallback to sklearn's MLP).\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# --- CONFIG ---\n",
    "CSV_PATH = \"/content/cleaned_pitstops.csv\"\n",
    "TARGET = \"Driver Aggression Score\"\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "RANDOM_STATE = 42\n",
    "SAVE_DIR = \"/tmp/f1_model\"   # change as needed\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- LOAD ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# drop rows missing target\n",
    "df = df[~df[TARGET].isna()].copy()\n",
    "print(\"After dropping missing target rows:\", df.shape)\n",
    "\n",
    "# Choose feature columns (exclude identifiers and target)\n",
    "# Customize drop list if needed\n",
    "drop_cols = ['Driver', 'Date', 'Time_of_race', 'Pit_Time', TARGET]\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "print(\"Using features:\", features)\n",
    "\n",
    "# --- Detect numeric vs categorical (simple heuristic) ---\n",
    "numeric_cols = []\n",
    "cat_cols = []\n",
    "for c in features:\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        # if column is convertible to numeric for >90% rows, treat numeric\n",
    "        coerced = pd.to_numeric(df[c], errors='coerce')\n",
    "        if coerced.notna().mean() > 0.9:\n",
    "            df[c] = coerced\n",
    "            numeric_cols.append(c)\n",
    "        else:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "print(\"Numeric:\", numeric_cols)\n",
    "print(\"Categorical:\", cat_cols)\n",
    "\n",
    "# Fill missing values: numbers -> median, categories -> 'MISSING'\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "df[cat_cols] = df[cat_cols].fillna('MISSING')\n",
    "\n",
    "# Preprocessing: scale numeric, one-hot encode categorical\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "X = df[features]\n",
    "y = df[TARGET].astype(float).values\n",
    "\n",
    "# Fit preprocessor and transform\n",
    "X_proc = preprocessor.fit_transform(X)\n",
    "print(\"Processed feature matrix shape:\", X_proc.shape)\n",
    "\n",
    "# Train/val/test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_proc, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "# keep a validation set from train_val\n",
    "val_fraction_of_train = VAL_SIZE / (1 - TEST_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_fraction_of_train, random_state=RANDOM_STATE)\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# --- Build Keras model (if TF available), else fallback to sklearn MLP ---\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "\n",
    "    def build_model(input_dim):\n",
    "        inp = layers.Input(shape=(input_dim,))\n",
    "        x = layers.Dense(512, activation=\"relu\")(inp)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "\n",
    "        x = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "        out = layers.Dense(1, activation=\"linear\")(x)\n",
    "        model = models.Model(inputs=inp, outputs=out)\n",
    "        return model\n",
    "\n",
    "    model = build_model(X_train.shape[1])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss=\"mse\", metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # callbacks\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1)\n",
    "    rlrop = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-6, verbose=1)\n",
    "    chkpt = callbacks.ModelCheckpoint(os.path.join(SAVE_DIR, \"best_model.h5\"), monitor=\"val_loss\", save_best_only=True, verbose=0)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=[es, rlrop, chkpt],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    y_pred = model.predict(X_test).ravel()\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"\\nTest MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "    # Save model and preprocessor\n",
    "    model.save(os.path.join(SAVE_DIR, \"keras_model\"))\n",
    "    joblib.dump(preprocessor, os.path.join(SAVE_DIR, \"preprocessor.joblib\"))\n",
    "    print(f\"Saved Keras model and preprocessor to {SAVE_DIR}\")\n",
    "\n",
    "    # --- Plots ---\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE Loss\"); plt.legend(); plt.title(\"Training / Validation Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # MAE\n",
    "    if \"mean_absolute_error\" in history.history:\n",
    "        train_mae_key = \"mean_absolute_error\"\n",
    "        val_mae_key = \"val_mean_absolute_error\"\n",
    "    else:\n",
    "        # Keras metric name sometimes \"mean_absolute_error\" or \"mean_absolute_error\"\n",
    "        train_mae_key = [k for k in history.history.keys() if \"mean_absolute\" in k and not k.startswith(\"val\")][0]\n",
    "        val_mae_key = [k for k in history.history.keys() if k.startswith(\"val\") and \"mean_absolute\" in k][0]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history[train_mae_key], label=\"train_mae\")\n",
    "    plt.plot(history.history[val_mae_key], label=\"val_mae\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MAE\"); plt.legend(); plt.title(\"Training / Validation MAE\")\n",
    "    plt.show()\n",
    "\n",
    "    # Pred vs Actual\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])\n",
    "    plt.xlabel(\"Actual Driver Aggression Score\"); plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predicted vs Actual (Test)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure()\n",
    "    plt.hist(residuals, bins=40)\n",
    "    plt.xlabel(\"Residual (Actual - Pred)\"); plt.title(\"Residuals Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback: sklearn MLP (useful if TF not installed)\n",
    "    print(\"TensorFlow Keras not available or failed. Falling back to sklearn MLPRegressor. Error:\", e)\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "    # Convert one-hot or categorical differently? We already used preprocessor above if created.\n",
    "    # For speed: use simpler preprocessing: convert categories to codes then scale\n",
    "    X_simple = df[features].copy()\n",
    "    # numeric/categorical detection repeated:\n",
    "    for c in X_simple.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(X_simple[c]):\n",
    "            # factorize category -> codes\n",
    "            X_simple[c] = X_simple[c].astype('category').cat.codes\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_simple)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=(TEST_SIZE + VAL_SIZE), random_state=RANDOM_STATE)\n",
    "    # split val off train\n",
    "    test_split = TEST_SIZE / (TEST_SIZE + VAL_SIZE)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_split, random_state=RANDOM_STATE)\n",
    "\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(256,128), max_iter=300, early_stopping=True, n_iter_no_change=10, random_state=RANDOM_STATE)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Fallback MLP Test MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "    # save fallback objects\n",
    "    joblib.dump(mlp, os.path.join(SAVE_DIR, \"mlp_model.joblib\"))\n",
    "    joblib.dump(scaler, os.path.join(SAVE_DIR, \"scaler.joblib\"))\n",
    "    print(f\"Saved fallback MLP and scaler to {SAVE_DIR}\")\n",
    "\n",
    "    # simple plots\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])\n",
    "    plt.title(\"Predicted vs Actual (Test) - Fallback MLP\"); plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(y_test - y_pred, bins=40)\n",
    "    plt.title(\"Residuals - Fallback MLP\"); plt.xlabel(\"Residual\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Useful metrics outputs ---\n",
    "print(\"\\nFinal numeric metrics: \")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R2:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Optional: discretized accuracy (if you want to measure class-like accuracy)\n",
    "try:\n",
    "    bins = np.linspace(y.min(), y.max(), 6)  # 5 bins\n",
    "    y_test_bins = np.digitize(y_test, bins)\n",
    "    y_pred_bins = np.digitize(y_pred, bins)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print(\"Discretized accuracy (5 bins):\", accuracy_score(y_test_bins, y_pred_bins))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"Script finished. Models and preprocessors saved to\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkjW8FjJO8B2",
    "outputId": "10194af6-dea0-4406-c3e6-e5f9681af79e"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(model, \"driver_aggression_model.pkl\")\n",
    "print(\"âœ… Model saved successfully as driver_aggression_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTn-9xHRQn5x",
    "outputId": "dc70744e-19aa-45d6-80ca-ced7f0b06328"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model to file\n",
    "joblib.dump(model, \"driver_aggression_model.pkl\")\n",
    "print(\"âœ… Model saved successfully as driver_aggression_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dG2FKXaYQwXm",
    "outputId": "464d87fd-7edb-44dc-920e-36c001ff5739"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# TABTRANSFORMER MODEL FOR DRIVER AGGRESSION SCORE PREDICTION\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1ï¸âƒ£ LOAD DATASET\n",
    "# ---------------------------------------------------------------\n",
    "df = pd.read_csv('/content/cleaned_pitstops.csv')\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2ï¸âƒ£ SPLIT NUMERIC & CATEGORICAL FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "target = 'Driver Aggression Score'\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target)\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3ï¸âƒ£ ENCODE CATEGORICAL FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "cat_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    cat_encoders[col] = le\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4ï¸âƒ£ SCALE NUMERIC FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5ï¸âƒ£ TRAIN-TEST SPLIT\n",
    "# ---------------------------------------------------------------\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Prepare inputs for TabTransformer\n",
    "X_train_num = X_train[numeric_cols].values\n",
    "X_test_num = X_test[numeric_cols].values\n",
    "X_train_cat = [X_train[col].values for col in cat_cols]\n",
    "X_test_cat = [X_test[col].values for col in cat_cols]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6ï¸âƒ£ DEFINE TABTRANSFORMER MODEL\n",
    "# ---------------------------------------------------------------\n",
    "def build_tabtransformer(num_numeric, cat_cardinalities, embed_dim=32, num_heads=4, num_layers=3):\n",
    "    inputs_num = layers.Input(shape=(num_numeric,), name=\"numeric\")\n",
    "\n",
    "    # Categorical inputs + embeddings\n",
    "    cat_inputs = [layers.Input(shape=(1,), name=f\"cat_{i}\") for i in range(len(cat_cardinalities))]\n",
    "    cat_embeddings = [\n",
    "        layers.Embedding(input_dim=card, output_dim=embed_dim)(inp)\n",
    "        for card, inp in zip(cat_cardinalities, cat_inputs)\n",
    "    ]\n",
    "\n",
    "    # Concatenate embeddings -> [batch, num_cats, embed_dim]\n",
    "    cat_features = layers.Concatenate(axis=1)(cat_embeddings)\n",
    "    if len(cat_features.shape) == 4:\n",
    "        cat_features = layers.Lambda(lambda x: tf.squeeze(x, axis=2))(cat_features)\n",
    "\n",
    "    # Project numeric features into same embedding space\n",
    "    num_proj = layers.Dense(embed_dim)(inputs_num)\n",
    "    num_proj = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(num_proj)  # Keras-safe expand\n",
    "\n",
    "    # Combine numeric + categorical tokens\n",
    "    tokens = layers.Concatenate(axis=1)([num_proj, cat_features])\n",
    "\n",
    "    # Transformer encoder layers\n",
    "    x = tokens\n",
    "    for _ in range(num_layers):\n",
    "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        x = layers.Add()([x, attn_out])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        ff = layers.Dense(embed_dim * 4, activation='relu')(x)\n",
    "        ff = layers.Dense(embed_dim)(ff)\n",
    "        x = layers.Add()([x, ff])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Pooling + Regression head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='linear', name='Driver_Aggression_Score')(x)\n",
    "\n",
    "    model = models.Model(inputs=[inputs_num] + cat_inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7ï¸âƒ£ BUILD AND COMPILE MODEL\n",
    "# ---------------------------------------------------------------\n",
    "cat_cardinalities = [len(cat_encoders[col].classes_) for col in cat_cols]\n",
    "model = build_tabtransformer(len(numeric_cols), cat_cardinalities)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(1e-3),\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 8ï¸âƒ£ TRAIN MODEL\n",
    "# ---------------------------------------------------------------\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_num] + X_train_cat, y_train,\n",
    "    validation_data=([X_test_num] + X_test_cat, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 9ï¸âƒ£ EVALUATE MODEL\n",
    "# ---------------------------------------------------------------\n",
    "results = model.evaluate([X_test_num] + X_test_cat, y_test, verbose=0)\n",
    "print(f\"\\nâœ… Test Loss (MSE): {results[0]:.4f}\")\n",
    "print(f\"âœ… Test MAE: {results[1]:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ðŸ”Ÿ VISUALIZE TRAINING PERFORMANCE\n",
    "# ---------------------------------------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mean_absolute_error'], label='Train MAE')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='Val MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.title('MAE over Epochs')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1ï¸âƒ£1ï¸âƒ£ SAVE MODEL + ENCODERS\n",
    "# ---------------------------------------------------------------\n",
    "model.save('driver_aggression_tabtransformer.h5')\n",
    "print(\"\\nâœ… Model saved as 'driver_aggression_tabtransformer.h5'\")\n",
    "\n",
    "with open('encoders_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump({'cat_encoders': cat_encoders, 'scaler': scaler}, f)\n",
    "\n",
    "print(\"âœ… Encoders & Scaler saved as 'encoders_scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xtkqw8AAVP_M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(history.history).to_csv(\"training_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "09qvRTTtYTLp",
    "outputId": "f931dbfd-4b42-465b-cdeb-2eaa3ce91f78"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_test_num] + X_test_cat).ravel()\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel(\"Actual Aggression\")\n",
    "plt.ylabel(\"Predicted Aggression\")\n",
    "plt.title(\"Predicted vs Actual Aggression Score\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RBfYXnnYVgP",
    "outputId": "7aeb48b9-5451-4d94-df70-10bc4f087b1c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(\"RÂ² Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ0bTZFzYYli"
   },
   "outputs": [],
   "source": [
    "model.save(\"driver_aggression_tabtransformer_v1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSxfOYmpn3gH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_tabtransformer_safe(num_numeric, cat_cardinalities, embed_dim=32, num_heads=4, num_layers=3):\n",
    "    inputs_num = layers.Input(shape=(num_numeric,), name=\"numeric\")\n",
    "\n",
    "    # categorical inputs + embeddings\n",
    "    cat_inputs = [layers.Input(shape=(1,), name=f\"cat_{i}\") for i in range(len(cat_cardinalities))]\n",
    "    cat_embeddings = [\n",
    "        layers.Embedding(input_dim=card, output_dim=embed_dim)(inp)\n",
    "        for card, inp in zip(cat_cardinalities, cat_inputs)\n",
    "    ]\n",
    "\n",
    "    # concatenate embeddings\n",
    "    cat_features = layers.Concatenate(axis=1)(cat_embeddings)\n",
    "    cat_features = layers.Lambda(\n",
    "        lambda x: tf.squeeze(x, axis=2),\n",
    "        output_shape=lambda s: (s[0], s[1], s[3])\n",
    "    )(cat_features) if len(cat_features.shape) == 4 else cat_features\n",
    "\n",
    "    # numeric projection\n",
    "    num_proj = layers.Dense(embed_dim)(inputs_num)\n",
    "    num_proj = layers.Lambda(\n",
    "        lambda x: tf.expand_dims(x, axis=1),\n",
    "        output_shape=lambda s: (s[0], 1, s[1])\n",
    "    )(num_proj)\n",
    "\n",
    "    # combine tokens\n",
    "    tokens = layers.Concatenate(axis=1)([num_proj, cat_features])\n",
    "\n",
    "    # transformer encoder\n",
    "    x = tokens\n",
    "    for _ in range(num_layers):\n",
    "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        x = layers.Add()([x, attn_out])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        ff = layers.Dense(embed_dim * 4, activation='relu')(x)\n",
    "        ff = layers.Dense(embed_dim)(ff)\n",
    "        x = layers.Add()([x, ff])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # pooling + regression\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='linear', name='Driver_Aggression_Score')(x)\n",
    "\n",
    "    model = models.Model(inputs=[inputs_num] + cat_inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQQtI7r4okGj",
    "outputId": "e337bed4-9d8d-493c-af25-9c80343f8552"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "SAVE_DIR = \"/tmp/f1_model\"\n",
    "\n",
    "# Save model in Keras 3 format\n",
    "model.save(f\"{SAVE_DIR}/driver_aggression_model.keras\")\n",
    "\n",
    "# Save the preprocessor\n",
    "joblib.dump(preprocessor, f\"{SAVE_DIR}/preprocessor.joblib\")\n",
    "print(\"âœ… Model and preprocessor saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4qxx11bpUbt"
   },
   "outputs": [],
   "source": [
    "def build_tabtransformer(num_numeric, cat_cardinalities, embed_dim=32, num_heads=4, num_layers=3):\n",
    "    inputs_num = layers.Input(shape=(num_numeric,), name=\"numeric\")\n",
    "\n",
    "    # Categorical inputs + embeddings\n",
    "    cat_inputs = [layers.Input(shape=(1,), name=f\"cat_{i}\") for i in range(len(cat_cardinalities))]\n",
    "    cat_embeddings = [\n",
    "        layers.Embedding(input_dim=card, output_dim=embed_dim)(inp)\n",
    "        for card, inp in zip(cat_cardinalities, cat_inputs)\n",
    "    ]\n",
    "\n",
    "    # Concatenate embeddings -> [batch, num_cats, embed_dim]\n",
    "    cat_features = layers.Concatenate(axis=1)(cat_embeddings)\n",
    "    if len(cat_features.shape) == 4:\n",
    "        cat_features = layers.Lambda(\n",
    "            lambda x: tf.squeeze(x, axis=2),\n",
    "            output_shape=lambda input_shape: (input_shape[0], input_shape[1], input_shape[3])\n",
    "        )(cat_features)\n",
    "\n",
    "    # Project numeric features into same embedding space\n",
    "    num_proj = layers.Dense(embed_dim)(inputs_num)\n",
    "    num_proj = layers.Lambda(\n",
    "        lambda x: tf.expand_dims(x, axis=1),\n",
    "        output_shape=lambda input_shape: (input_shape[0], 1, input_shape[1])\n",
    "    )(num_proj)\n",
    "\n",
    "    # Combine numeric + categorical tokens\n",
    "    tokens = layers.Concatenate(axis=1)([num_proj, cat_features])\n",
    "\n",
    "    # Transformer encoder layers\n",
    "    x = tokens\n",
    "    for _ in range(num_layers):\n",
    "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        x = layers.Add()([x, attn_out])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        ff = layers.Dense(embed_dim * 4, activation='relu')(x)\n",
    "        ff = layers.Dense(embed_dim)(ff)\n",
    "        x = layers.Add()([x, ff])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Pooling + Regression head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='linear', name='Driver_Aggression_Score')(x)\n",
    "\n",
    "    model = models.Model(inputs=[inputs_num] + cat_inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssytt4rypuXj"
   },
   "outputs": [],
   "source": [
    "model.save(f\"{SAVE_DIR}/driver_aggression_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCfHW8RzqOMF",
    "outputId": "37076186-c2f4-4782-f0c2-8700d1b8ab6d"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# ðŸ”§ FIXED REBUILD CODE TO LOAD OLD WEIGHTS SAFELY\n",
    "# ===============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pickle\n",
    "\n",
    "# 1ï¸âƒ£ Reload your dataset and metadata\n",
    "df = pd.read_csv('/content/cleaned_pitstops.csv')\n",
    "\n",
    "target = 'Driver Aggression Score'\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target)\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Load encoders/scaler to get categorical sizes\n",
    "with open('encoders_scaler.pkl', 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "cat_encoders = obj['cat_encoders']\n",
    "scaler = obj['scaler']\n",
    "\n",
    "cat_cardinalities = [len(cat_encoders[col].classes_) for col in cat_cols]\n",
    "num_numeric = len(numeric_cols)\n",
    "\n",
    "# 2ï¸âƒ£ Rebuild architecture (with output_shape fixes)\n",
    "def build_tabtransformer(num_numeric, cat_cardinalities, embed_dim=32, num_heads=4, num_layers=3):\n",
    "    inputs_num = layers.Input(shape=(num_numeric,), name=\"numeric\")\n",
    "    cat_inputs = [layers.Input(shape=(1,), name=f\"cat_{i}\") for i in range(len(cat_cardinalities))]\n",
    "\n",
    "    cat_embeddings = [\n",
    "        layers.Embedding(input_dim=card, output_dim=embed_dim)(inp)\n",
    "        for card, inp in zip(cat_cardinalities, cat_inputs)\n",
    "    ]\n",
    "\n",
    "    cat_features = layers.Concatenate(axis=1)(cat_embeddings)\n",
    "    if len(cat_features.shape) == 4:\n",
    "        cat_features = layers.Lambda(\n",
    "            lambda x: tf.squeeze(x, axis=2),\n",
    "            output_shape=lambda s: (s[0], s[1], s[3])\n",
    "        )(cat_features)\n",
    "\n",
    "    num_proj = layers.Dense(embed_dim)(inputs_num)\n",
    "    num_proj = layers.Lambda(\n",
    "        lambda x: tf.expand_dims(x, axis=1),\n",
    "        output_shape=lambda s: (s[0], 1, s[1])\n",
    "    )(num_proj)\n",
    "\n",
    "    tokens = layers.Concatenate(axis=1)([num_proj, cat_features])\n",
    "\n",
    "    x = tokens\n",
    "    for _ in range(num_layers):\n",
    "        attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "        x = layers.Add()([x, attn_out])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        ff = layers.Dense(embed_dim * 4, activation='relu')(x)\n",
    "        ff = layers.Dense(embed_dim)(ff)\n",
    "        x = layers.Add()([x, ff])\n",
    "        x = layers.LayerNormalization()(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = models.Model(inputs=[inputs_num] + cat_inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# 3ï¸âƒ£ Rebuild and load your weights\n",
    "model = build_tabtransformer(num_numeric, cat_cardinalities)\n",
    "model.load_weights(\"/content/driver_aggression_tabtransformer.h5\")\n",
    "\n",
    "# 4ï¸âƒ£ Save in new format (no shape inference issues)\n",
    "SAVE_DIR = \"/tmp/f1_model\"\n",
    "model.save(f\"{SAVE_DIR}/driver_aggression_model.keras\")\n",
    "\n",
    "print(\"âœ… Fixed model successfully rebuilt and saved to /tmp/f1_model/driver_aggression_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mao9BEZcqZ6K"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f\"{SAVE_DIR}/driver_aggression_model.keras\")\n",
    "preprocessor = joblib.load(f\"{SAVE_DIR}/preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xafdoebjqwwB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load dataset again\n",
    "df = pd.read_csv('/content/cleaned_pitstops.csv')\n",
    "target = 'Driver Aggression Score'\n",
    "\n",
    "# Separate columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(target)\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Load encoders and scaler\n",
    "with open('encoders_scaler.pkl', 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "cat_encoders = obj['cat_encoders']\n",
    "scaler = obj['scaler']\n",
    "\n",
    "# Apply encoders + scaler again (same preprocessing)\n",
    "for col in cat_cols:\n",
    "    df[col] = cat_encoders[col].transform(df[col].astype(str))\n",
    "df[numeric_cols] = scaler.transform(df[numeric_cols])\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oSoik42rV2Z",
    "outputId": "7fcbc3e9-7886-43a2-9e60-bf352c89283e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "SAVE_DIR = \"/tmp/f1_model\"\n",
    "\n",
    "# âœ… Reload the model safely, giving Keras a reference to TensorFlow\n",
    "custom_objects = {\"tf\": tf}\n",
    "model = keras.models.load_model(\n",
    "    f\"{SAVE_DIR}/driver_aggression_model.keras\",\n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "print(\"âœ… Model reloaded successfully with TensorFlow bound to Lambda layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXOOqI6yrKFY",
    "outputId": "3dce5e00-89fd-479e-ff61-06c6504bc6e9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "SAVE_DIR = \"/tmp/f1_model\"\n",
    "\n",
    "# Explicitly give TensorFlow reference to Keras during deserialization\n",
    "model = keras.models.load_model(\n",
    "    f\"{SAVE_DIR}/driver_aggression_model.keras\",\n",
    "    custom_objects={\"tf\": tf}\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully with TensorFlow bound.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbViJuIys1WU"
   },
   "outputs": [],
   "source": [
    "# Preprocessor should already be loaded\n",
    "# Example: preprocessor = joblib.load(f\"{SAVE_DIR}/preprocessor.joblib\")\n",
    "\n",
    "def model_predict(X):\n",
    "    # Make sure X is transformed before prediction\n",
    "    X_proc = preprocessor.transform(X)\n",
    "    preds = model.predict(X_proc)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d698122f79e54ab3b0bc22b6f97ba5a6",
      "8e806ef2b74742ec81237189bb449517",
      "a26996137cf34d31b296cc944733aeb8",
      "fb24ee6cc28f4f00985cd1f4efe0c67e",
      "08d9da15be384d5cbb2587173a2de439",
      "5b93437a59ee4e9a9c624b6f419d68fc",
      "330759d98f194be1806537c1111daa2b",
      "08c91f097dd1438a99cd68f4705783f1",
      "9f69529d23694257a5821b51578a9284",
      "79bbb3cffd7b4bfd9ff5d2c13aa7a0c8",
      "946d021e366842a1a9475e3944813ae3"
     ]
    },
    "id": "zgRIxjugtYy0",
    "outputId": "0768ff66-4906-42fa-c7d2-f96e44d3d85c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "# Load saved encoders + scaler\n",
    "# -------------------------------\n",
    "with open(\"encoders_scaler.pkl\", \"rb\") as f:\n",
    "    enc_dict = pickle.load(f)\n",
    "\n",
    "cat_encoders = enc_dict['cat_encoders']\n",
    "scaler = enc_dict['scaler']\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare sample data\n",
    "# -------------------------------\n",
    "X_sample = X_test.sample(200, random_state=42).copy()\n",
    "\n",
    "# Encode categorical columns\n",
    "for col, le in cat_encoders.items():\n",
    "    X_sample[col] = X_sample[col].astype(str)\n",
    "    # Replace unseen labels with a known one (to avoid errors)\n",
    "    X_sample[col] = X_sample[col].apply(lambda x: x if x in le.classes_ else le.classes_[0])\n",
    "    X_sample[col] = le.transform(X_sample[col])\n",
    "\n",
    "# Scale numeric columns\n",
    "X_sample[numeric_cols] = scaler.transform(X_sample[numeric_cols])\n",
    "\n",
    "# Convert to numpy for model\n",
    "X_num = X_sample[numeric_cols].values\n",
    "X_cat = [X_sample[col].values for col in cat_encoders.keys()]\n",
    "X_combined = [X_num] + X_cat\n",
    "\n",
    "print(\"âœ… Manual preprocessing completed:\", X_num.shape, \"numeric +\", len(X_cat), \"categorical\")\n",
    "\n",
    "# -------------------------------\n",
    "# SHAP explainability\n",
    "# -------------------------------\n",
    "print(\"âš™ï¸ Running SHAP explainability (may take 2â€“3 min)...\")\n",
    "\n",
    "# Use a small background for speed\n",
    "X_bg_num = X_num[:50]\n",
    "X_bg_cat = [x[:50] for x in X_cat]\n",
    "X_bg = [X_bg_num] + X_bg_cat\n",
    "\n",
    "# Wrap prediction\n",
    "def model_predict(data):\n",
    "    num_features = data[:, :X_num.shape[1]]\n",
    "    cat_features = [data[:, X_num.shape[1] + i] for i in range(len(X_cat))]\n",
    "    return model.predict([num_features] + cat_features)\n",
    "\n",
    "# Combine numeric + categorical for SHAP\n",
    "X_shap = np.hstack([X_num] + [c.reshape(-1, 1) for c in X_cat])\n",
    "\n",
    "# Run SHAP\n",
    "explainer = shap.KernelExplainer(model_predict, X_shap[:50])\n",
    "shap_values = explainer.shap_values(X_shap[:100])\n",
    "\n",
    "# -------------------------------\n",
    "# Visualize\n",
    "# -------------------------------\n",
    "shap.summary_plot(shap_values, X_shap)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8TAAQjZh5_H8",
    "outputId": "69f960cc-4a78-408d-bd2a-8edcf76f9929"
   },
   "outputs": [],
   "source": [
    "print(\"shap_values shape:\", np.array(shap_values).shape)\n",
    "print(\"X_shap shape:\", X_shap_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PApYccpz6Hzt",
    "outputId": "e420c65b-1bb3-4171-d057-ea16b402cdb6"
   },
   "outputs": [],
   "source": [
    "shap_values = np.squeeze(shap_values, axis=-1)\n",
    "\n",
    "# Now visualize\n",
    "shap.summary_plot(shap_values, X_shap_sample)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-JRNrqN8m0M",
    "outputId": "7010a5ff-d8ee-4555-827d-4c1bc85c9e94"
   },
   "outputs": [],
   "source": [
    "# âœ… Save model\n",
    "model.save(\"final_model.h5\")\n",
    "print(\"âœ… Model saved as final_model.h5\")\n",
    "\n",
    "# âœ… Load model later\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"final_model.h5\")\n",
    "print(\"âœ… Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1zesPzb9OfX",
    "outputId": "dfb36690-001a-4a80-da3b-527c98c63b01"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# âœ… Recover feature names from your preprocessor\n",
    "# --------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "# Numerical and categorical feature lists\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][2]\n",
    "\n",
    "# Get encoded categorical feature names\n",
    "cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "cat_encoded_names = list(cat_encoder.get_feature_names_out(cat_features))\n",
    "\n",
    "# Combine them all\n",
    "all_feature_names = list(num_features) + cat_encoded_names\n",
    "\n",
    "print(\"ðŸ§© Total processed features:\", len(all_feature_names))\n",
    "print(\"ðŸ”¹ First 15 feature names:\\n\", all_feature_names[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnYr9NqkIEzz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
